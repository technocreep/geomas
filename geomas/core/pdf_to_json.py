import json
import os
import re
import subprocess
import unicodedata

from bs4 import BeautifulSoup
from PyPDF2 import PdfReader

from geomas.core.logger import get_logger

logger = get_logger()


def pdf_to_text(pdf_path):
    reader = PdfReader(pdf_path)
    text = ""
    for page in reader.pages:
        page_text = page.extract_text()
        if page_text:
            text += page_text + "\n"
    return " ".join(text.split())


def djvu_to_text(djvu_path):
    txt_path = djvu_path + ".txt"
    subprocess.run(["djvutxt", djvu_path, txt_path], check=True)
    with open(txt_path, "r", encoding="utf-8") as f:
        text = f.read()
    os.remove(txt_path)
    return " ".join(text.split())


def html_to_text(html_path):
    with open(html_path, "r", encoding="utf-8") as f:
        soup = BeautifulSoup(f, "html.parser")
    text = soup.get_text(separator=" ")
    return " ".join(text.split())

def txt_to_text(txt_path):
    with open(txt_path, 'r') as f:
        data = f.read()

    return data


def chunk_text_by_sentences(text, max_chunk_size=1200):
    sentences = text.split(".")
    chunks = []
    current_chunk = ""

    for sentence in sentences:
        sentence = sentence.strip()
        if not sentence:
            continue
        sentence_with_dot = sentence + "."
        if len(current_chunk) + len(sentence_with_dot) > max_chunk_size:
            chunks.append(current_chunk.strip())
            current_chunk = sentence_with_dot
        else:
            current_chunk += (
                " " + sentence_with_dot if current_chunk else sentence_with_dot
            )

    if current_chunk:
        chunks.append(current_chunk.strip())

    return chunks


def clean_text(text: str) -> str:
    """Cleans and normalizes text after extraction"""

    # 1. Gluing word hyphenation
    text = re.sub(r"(\w+)-\s+(\w+)", r"\1\2", text)

    # 2. Replace multiple spaces into single space
    text = re.sub(r"\s+", " ", text)

    # 3. Remove spaces before punctuation
    text = re.sub(r"\s+([,.;:!?])", r"\1", text)

    # 4. Remove spaces after brackets
    text = re.sub(r"\(\s+", "(", text)
    text = re.sub(r"\s+\)", ")", text)

    # 5. Bring quotes to the same type
    text = text.replace("¬´", '"').replace("¬ª", '"')

    # 6. Unicode normalization
    text = unicodedata.normalize("NFC", text)

    # 7. Clean up extra spaces at the edges
    text = text.strip()

    return text


def save_to_json(chunks, json_path):
    os.makedirs(os.path.dirname(json_path), exist_ok=True)
    data = [{"text": chunk} for chunk in chunks if chunk]
    with open(json_path, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)


def call_ollama(txt: str) -> dict:
    import requests
    system = """
    –¢—ã ‚Äî –¥–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π ¬´Text Cleaner¬ª. –ó–∞–¥–∞—á–∞ ‚Äî –ù–ò–ö–ê–ö–ò–• –¥–æ–º—ã—Å–ª–æ–≤.

    1) –ù–µ –¥–æ–±–∞–≤–ª—è–π –∏ –Ω–µ —É–¥–∞–ª—è–π —Ñ–∞–∫—Ç–∏—á–µ—Å–∫–æ–µ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ, —Ç–æ–ª—å–∫–æ —á–∏—Å—Ç—å —Ñ–æ—Ä–º–∞—Ç.
    2) –í–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–π —Ä–∞–∑—Ä—ã–≤—ã —Å–ª–æ–≤ –∏ —Å—Ç—Ä–æ–∫, —É–¥–∞–ª—è–π –∫–æ–ª–æ–Ω—Ç–∏—Ç—É–ª—ã/–Ω–æ–º–µ—Ä–∞ —Å—Ç—Ä–∞–Ω–∏—Ü/–ª–∏–Ω–∏–∏-—Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª–∏.
    3) –ù–æ—Ä–º–∞–ª–∏–∑—É–π —é–Ω–∏–∫–æ–¥ (NFKC), –∑–∞–º–µ–Ω—ã: ‚Äúsmart quotes‚Äù‚Üí", ‚Äò ‚Äô‚Üí', ligatures (Ô¨Å‚Üífi), –Ω–µ—Ä–∞–∑—Ä—ã–≤–Ω—ã–µ –ø—Ä–æ–±–µ–ª—ã‚Üí–æ–±—ã—á–Ω—ã–µ.
    4) –£–±–µ—Ä–∏ –ø–µ—Ä–µ–Ω–æ—Å—ã –ø–æ –¥–µ—Ñ–∏—Å—É –Ω–∞ –≥—Ä–∞–Ω–∏—Ü–µ —Å—Ç—Ä–æ–∫: "–º–∏–∫—Ä–æ-\n—Å—Ç—Ä—É–∫—Ç—É—Ä–∞"‚Üí"–º–∏–∫—Ä–æ—Å—Ç—Ä—É–∫—Ç—É—Ä–∞".
    5) –°–æ—Ö—Ä–∞–Ω–∏ —Å–ø–∏—Å–∫–∏ –∏ –∑–∞–≥–æ–ª–æ–≤–∫–∏ –∫–∞–∫ Markdown (#, ##, -, 1.), —Ç–∞–±–ª–∏—Ü—ã ‚Äî –∫–∞–∫ pipe-—Ç–∞–±–ª–∏—Ü—ã –±–µ–∑ –≥—Ä–∞—Ñ–∏–∫–∏.
    6) –°–Ω–æ—Å–∫–∏/—Ü–∏—Ç–∞—Ç—ã –≤–∏–¥–∞ [1], (1) ‚Äî —É–±–µ—Ä–∏ –∏–∑ —Ç–µ–∫—Å—Ç–∞; –∫–æ–ª–æ–Ω—Ç–∏—Ç—É–ª—ã/–Ω–æ–º–µ—Ä–∞ —Å—Ç—Ä–∞–Ω–∏—Ü/—Å–∫–∞–Ω-–º–µ—Ç–∫–∏ ‚Äî —É–¥–∞–ª–∏—Ç—å.
    7) –ú–∞—Ç–µ–º–∞—Ç–∏–∫—É –∏ —Ñ–æ—Ä–º—É–ª—ã –Ω–µ –ø–µ—Ä–µ–ø–∏—Å—ã–≤–∞—Ç—å: –æ—Å—Ç–∞–≤—å –∫–∞–∫ –µ—Å—Ç—å, LaTeX –Ω–µ —Ä–∞—Å—à–∏—Ä—è—Ç—å.
    8) –Ø–∑—ã–∫ –∏—Å—Ç–æ—á–Ω–∏–∫–∞ —Å–æ—Ö—Ä–∞–Ω—è–π (—Ä—É—Å/–∞–Ω–≥–ª –∏ —Ç.–ø.).
    9) –°–æ–±–∏—Ä–∞—Ç—å —Å–ª–æ–≤–∞, —Ä–∞–∑–±–∏—Ç—ã–µ –Ω–∞ –±—É–∫–≤—ã –≤–æ–µ–¥–∏–Ω–æ.
    10) –£–±–∏—Ä–∞—Ç—å —Ç–æ, —á—Ç–æ —è–≤–ª—è–µ—Ç—Å—è –ø–æ–¥–ø–∏—Å—å—é –∫ —Ä–∏—Å—É–Ω–∫—É.
    11) –£–±—Ä–∞—Ç—å —Å—Å—ã–ª–∫–∏ –Ω–∞ —Ä–∏—Å—É–Ω–∫–∏, –≥–ª–∞–≤—ã, —Ä–∞–∑–¥–µ–ª—ã, –∞–≤—Ç–æ—Ä–æ–≤, –ª–∏—Ç–µ—Ä–∞—Ç—É—Ä–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏
    12) –£–¥–∞–ª–∏—Ç—å –ø–µ—Ä–µ–Ω–æ—Å —Å—Ç—Ä–æ–∫–∏ –≤ —Å–µ—Ä–µ–¥–∏–Ω–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è.
    13) –£–¥–∞–ª–∏—Ç—å —Å—Å—ã–ª–∫–∏ –Ω–∞ —É—á–µ–Ω—ã—Ö –∏ –∏—Ö —Ä–∞–±–æ—Ç—ã.
    14) –£–¥–∞–ª–∏—Ç—å —Ç–µ–∫—Å—Ç –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤.

    –ü—Ä–∏–º–µ—Ä—ã —è–≤–Ω—ã—Ö –æ—à–∏–±–æ–∫ –≤ —Ç–µ–∫—Å—Ç–µ –∫–æ—Ç–æ—Ä—ã–µ –Ω—É–∂–Ω–æ –∏—Å–ø—Ä–∞–≤–∏—Ç—å:
    *–ë–µ—Å—Å–≤—è–∑–Ω–∞—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å*
    ‚Äì 2 10\"5 Li 1,5-2 10 Sr 8-–Æ\"4 Be 610\"11 Y 3 10\‚Äù

    *–ó–∞–≥–æ–ª–æ–≤–æ–∫ —Å—Ä–µ–¥–∏ —Ç–µ–∫—Å—Ç–∞*
    ‚Äì 30 –ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è –≥–µ–æ—Ö–∏–º–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö –ú–æ–∂–Ω–æ —Ä–∞—Å—Å—á–∏—Ç–∞—Ç—å
    ‚Äì –ü–æ —ç—Ç–æ–π –ø—Ä–∏—á–∏–Ω–µ –ø—Ä–∏ –Ω–∏–∑–∫–æ–π –ì–ª–∞–≤–∞ 1 ‚Ä¢ –ë–∞–∑–æ–≤—ã–µ –ø–æ–Ω—è—Ç–∏—è –∏ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –≤ –≥–µ–æ—Ö–∏–º–∏–∏ 31 
    ‚Äì –æ—Å–∞–¥–æ—á–Ω—ã—Ö –ø–æ—Ä–æ–¥. 1. 4. 3. –§–∞–∫—Ç–æ—Ä—ã, –æ–ø—Ä–µ–¥–µ–ª—è—é—â–∏–µ –≥–µ–æ—Ö–∏–º–∏—á–µ—Å–∫—É—é —Å–ø–µ—Ü–∏—Ñ–∏–∫—É –º–µ—Ç–∞–º–æ—Ä—Ñ–∏—á–µ—Å–∫–∏—Ö –ø–æ—Ä–æ–¥ –û–ø—Ä–µ–¥–µ–ª—è—é—â—É—é —Ä–æ–ª—å –≤ –≥–µ–æ—Ö–∏–º–∏—á–µ—Å–∫–∏—Ö –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç—è—Ö


    *–ù–µ–Ω—É–∂–Ω–∞—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å —á–∏—Å–µ–ª*
    ‚Äì 3,060 0,071 0,042 0,081 1,830 0,012 1,090 0,230 0,023 0,170

    *–°—Å—ã–ª–∫–∏ –Ω–∞ –ª–∏—Ç–µ—Ä–∞—Ç—É—Ä–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏*
    ‚Äì –¥–ª—è –∞–Ω–¥–µ–∑–∏—Ç–æ–≤—ã—Ö —Ä–∞—Å–ø–ª–∞–≤–æ–≤ [220] –†–æ–≥–æ–≤–∞—è –æ–±–º–∞–Ω–∫–∞ [144; 214]

    *–†–∞–∑–¥–µ–ª–µ–Ω–∏–µ —Å–ª–æ–≤–∞ –Ω–∞ –±—É–∫–≤—ã*
    ‚Äì pea –∫ —Ü–∏ –∏ –≥–∏–¥—Ä–∞—Ç–∞ —Ü –∏ –∏ /–¥–µ –≥–∏–¥—Ä–∞—Ç–∞ —Ü –∏ –∏

    *–ü–æ–¥–ø–∏—Å—å —Ä–∏—Å—É–Ω–∫–∞*
    ‚Äì –ò–∑–æ—Ö–∏–º–∏—á–µ—Å–∫–∏–µ –∏–∑–º–µ–Ω–µ–Ω–∏—è 1–∏—Ç 2 | | –ü—Ä–æ—Ç–æ–ª–∏—Ç | –†–∏—Å. 1. 12. –î–∏–∞–≥—Ä–∞–º–º–∞, –∏–ª–ª—é—Å—Ç—Ä–∏—Ä—É—é—â–∞—è –æ—Å–Ω–æ–≤–Ω—ã–µ –ø—Ä–æ—Ü–µ—Å—Å—ã,

    *–õ–∏—à–Ω–∏–µ —Å–∏–º–≤–æ–ª—ã*
    ‚Äì –æ–∫–æ–Ω—á–∞—Ç–µ–ª—å–Ω–æ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç–µ. * * * 44 –ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è

    *–°—Å—ã–ª–∫–∞ –Ω–∞ –≥–ª–∞–≤—É –≤ –∫–Ω–∏–≥–µ*
    ‚Äì –¥–∞–Ω–Ω—ã—Ö –ü–æ–¥–≤–æ–¥—è –∏—Ç–æ–≥ –≤—Å–µ–º—É –∏–∑–ª–æ–∂–µ–Ω–Ω–æ–º—É –≤ –≥–ª. 1,

    *–ë–æ–ª—å—à–∏–µ –±—É–∫–≤—ã*
    ‚Äì –ò–°–ü–û–õ–¨–ó–û–í–ê–ù–ò–ï –ì–ï–û–•–ò–ú–ò–ß–ï–°–ö–ò–• –î–ê–ù–ù–´–• –ü–†–ò –ò–ó–£–ß–ï–ù–ò–ò –ú–ê–ì–ú–ê–¢–ò–ß–ï–°–ö–ò–• –ü–û–†–û–î

    *–æ—à–∏–±–∫–∞ –≤ –æ–±–æ–∑–Ω–∞—á–µ–Ω–∏–∏ —Ö–∏–º–∏—á–µ—Å–∫–æ–≥–æ –≤–µ—â–µ—Å—Ç–≤–∞*
    ‚Äì (Na20 + –ö^–û) - Si02

    *–°—Å—ã–ª–∫–∞ –Ω–∞ —Ä–∏—Å—É–Ω–æ–∫*
    ‚Äì –õ–∏–Ω–∏—è –Ω–∞ –¥–∏–∞–≥—Ä–∞–º–º–µ (—Ä–∏—Å. 2. 4) —Ä–∞–∑–¥–µ–ª—è–µ—Ç –ø–æ—Ä–æ–¥—ã

    *—Å—Å—ã–ª–∫–∞ –Ω–∞ –∞–≤—Ç–æ—Ä–∞*
    - –∫–æ–Ω—Ç–∏–Ω–µ–Ω—Ç–∞–ª—å–Ω—ã—Ö –æ–∫—Ä–∞–∏–Ω–∞—Ö (–¢–∞—É—Å–æ–Ω, 1977) 

    –í –∫–∞—á–µ—Å—Ç–≤–µ –æ—Ç–≤–µ—Ç–∞ –≤–µ—Ä–Ω–∏ –¢–û–õ–¨–ö–û –ò–°–ü–†–ê–í–õ–ï–ù–ù–´–ô –¢–ï–ö–°–¢ 
    """

    OLLAMA_URL = "http://localhost:11434/api/generate"
    MODEL = "pdf-cleaner"

    r = requests.post(OLLAMA_URL, json={"model": MODEL, "prompt": txt, "stream": False, "options": {"seed": 42}})
    r.raise_for_status()
    out = r.json()["response"].strip()
    return out.strip('`')


def clean_chunks(chunks):
    from tqdm import tqdm

    cleaned_chunks = []
    
    with tqdm(total=len(chunks), desc='Cleaning chunks') as pbar:
        for chunk in chunks :
            cleaned_chunk = call_ollama(chunk)
            if 'NOTHING' in cleaned_chunk:
                cleaned_chunk = ""
            cleaned_chunks.append(cleaned_chunk)
            pbar.update(1)
    return cleaned_chunks

def process_folder(folder_path, output_folder="output_json", max_chunk_size=1200):
    for root, _, files in os.walk(folder_path):
        for filename in files:
            file_path = os.path.join(root, filename)

            expected_json = os.path.join(
                output_folder,                                  # root 
                os.path.relpath(root, folder_path),             # subfolder
                file_path.split('/')[-1].split('.')[0]+'.json'  # filename.json
                )
            # check existance
            if os.path.exists(expected_json):
                # check content
                with open(expected_json, 'r') as f:
                    data = json.load(f)
                if len(data) > 0:
                    logger.info(f"‚ùó File {file_path} already JSONed. Pass.")
                    continue
                else:
                    logger.info(f"ü•≤ File {file_path} already JSONed but empty. Replacing.")

            ext = os.path.splitext(filename)[1].lower()
            try:
                if ext == ".pdf":
                    text = pdf_to_text(file_path)
                elif ext == ".djvu":
                    text = djvu_to_text(file_path)
                elif ext in (".html", ".htm"):
                    text = html_to_text(file_path)
                elif ext == ".txt":
                    text = txt_to_text(file_path)
                else:
                    logger.info(f"Skipping {filename} (unsupported format)")
                    continue

                text = clean_text(text)
                chunks = chunk_text_by_sentences(text, max_chunk_size=max_chunk_size)

                cleaned_chunks = clean_chunks(chunks)

                if len(cleaned_chunks) == 0:
                    raise ValueError('Nothing extracted from source')

                # save JSON into the same folder structure
                rel_path = os.path.relpath(root, folder_path)
                out_dir = os.path.join(output_folder, rel_path)
                json_filename = os.path.splitext(filename)[0] + ".json"
                json_path = os.path.join(out_dir, json_filename)

                save_to_json(cleaned_chunks, json_path)
                # save_to_json(chunks, json_path)
                logger.info(f"‚úÖ {file_path} ‚Üí {json_path} ({len(cleaned_chunks)} chunks)")
                # logger.info(f"‚úÖ {file_path} ‚Üí {json_path} ({len(chunks)} chunks)")

            except Exception as e:
                logger.info(f"‚ùå Processing error {file_path}: {e}")


if __name__ == "__main__":
    # input_folder = "/app/one_source"
    process_folder(folder_path='/app/geo_sources_stage_1_cutted',
                   output_folder='/app/geo_sources_stage_1_cutted_output'
                   )
